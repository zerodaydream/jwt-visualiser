from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from typing import Dict, Any, List, AsyncGenerator
from app.llm.base import LLMProvider
import json

class GeminiAdapter(LLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.5-flash"):
        """
        Initializes the Gemini adapter.
        
        Args:
            api_key: The Google Cloud API key.
            model: Defaults to 'gemini-2.5-flash' for high speed and low cost.
        """
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=api_key, 
            model=model, 
            temperature=0.3,
            convert_system_message_to_human=True # Gemini handles system prompts differently sometimes, this smooths it out
        )

    async def generate_response(self, prompt: str, context: Dict[str, Any]) -> str:
        # Construct a safe context string strictly from the JWT data
        jwt_data = context.get('jwt_data', {})
        kb_context = context.get('knowledge_base', [])
        conversation_history = context.get('conversation_history', [])
        
        # Filter out RAG disabled messages
        kb_context = [doc for doc in kb_context if not doc.startswith("(RAG Disabled")]
        
        context_str = (
            f"JWT Header: {jwt_data.get('header', {})}\n"
            f"JWT Payload: {jwt_data.get('payload', {})}\n"
            f"Signature Present: {jwt_data.get('signature_present', False)}\n"
        )
        
        if kb_context:
            context_str += f"\nKnowledge Base Context: {kb_context}"
        
        system_prompt = (
            "You are a Senior Security Engineer and JWT Expert with deep knowledge of RFC 7519 (JWT), "
            "RFC 7515 (JWS), and common security vulnerabilities. "
            "You are analyzing a JSON Web Token. "
            "Answer the user's question accurately based on the provided token context. "
            "Use your expert knowledge of JWT standards, common claims (sub, iss, exp, iat, aud, etc.), "
            "and security best practices. "
            "If a claim is present in the token, explain it clearly. "
            "If asked about security, mention algorithm weaknesses (none, HS256 vs RS256), "
            "expiration checks, and signature validation. "
            "Do not hallucinate claims that are not in the token data provided. "
            "Keep answers concise, technical, and helpful. "
            "Maintain conversation context from previous exchanges."
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"Token Context:\n{context_str}")
        ]
        
        # Add conversation history
        for msg in conversation_history:
            if msg['role'] == 'user':
                messages.append(HumanMessage(content=msg['content']))
            elif msg['role'] == 'assistant':
                messages.append(AIMessage(content=msg['content']))
        
        # Add current question
        messages.append(HumanMessage(content=prompt))

        response = await self.llm.ainvoke(messages)
        return response.content
    
    async def generate_response_stream(self, prompt: str, context: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """
        Generates a streaming response for real-time display.
        Yields text chunks as they are generated by the model.
        """
        # Build the same context as non-streaming
        jwt_data = context.get('jwt_data', {})
        kb_context = context.get('knowledge_base', [])
        conversation_history = context.get('conversation_history', [])
        
        # Filter out RAG disabled messages
        kb_context = [doc for doc in kb_context if not doc.startswith("(RAG Disabled")]
        
        context_str = (
            f"JWT Header: {jwt_data.get('header', {})}\n"
            f"JWT Payload: {jwt_data.get('payload', {})}\n"
            f"Signature Present: {jwt_data.get('signature_present', False)}\n"
        )
        
        if kb_context:
            context_str += f"\nKnowledge Base Context: {kb_context}"
        
        system_prompt = (
            "You are a Senior Security Engineer and JWT Expert with deep knowledge of RFC 7519 (JWT), "
            "RFC 7515 (JWS), and common security vulnerabilities. "
            "You are analyzing a JSON Web Token. "
            "Answer the user's question accurately based on the provided token context. "
            "Use your expert knowledge of JWT standards, common claims (sub, iss, exp, iat, aud, etc.), "
            "and security best practices. "
            "If a claim is present in the token, explain it clearly. "
            "If asked about security, mention algorithm weaknesses (none, HS256 vs RS256), "
            "expiration checks, and signature validation. "
            "Do not hallucinate claims that are not in the token data provided. "
            "Keep answers concise, technical, and helpful. "
            "Maintain conversation context from previous exchanges."
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"Token Context:\n{context_str}")
        ]
        
        # Add conversation history
        for msg in conversation_history:
            if msg['role'] == 'user':
                messages.append(HumanMessage(content=msg['content']))
            elif msg['role'] == 'assistant':
                messages.append(AIMessage(content=msg['content']))
        
        # Add current question
        messages.append(HumanMessage(content=prompt))

        # Stream the response
        try:
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    yield chunk.content
        except Exception as e:
            # Fallback to error message if streaming fails
            yield f"Error: {str(e)}"